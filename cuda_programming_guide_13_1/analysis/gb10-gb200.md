# NVIDIA GB10 vs GB200 NVL72: Blackwell Family Comparison

The GB10 and GB200 NVL72 represent two extremes of NVIDIA's Blackwell architecture deployment spectrum. While the GB10 brings Blackwell capabilities to the desktop in a compact 140W package, the GB200 NVL72 is NVIDIA's flagship AI supercomputer rack featuring 72 Blackwell GPUs and 36 Grace CPUs designed for the most demanding large-scale AI training and inference workloads at datacenter scale.

## Core Specification Comparison

| Feature | NVIDIA GB10 | NVIDIA GB200 NVL72 |
|---------|-------------|---------------------|
| Release Date | Q4 2025 | Q4 2025 |
| Architecture | Blackwell (SM 12.1) | Hopper/Blackwell (SM 10.0) |
| Compute Capability | 12.1 | 10.0 |
| GPU Count | 1 GPU | 72 GPUs |
| CPU Count | 1 Grace CPU (20-core) | 36 Grace CPUs |
| Streaming Multiprocessors | ~40 SMs | ~10,368 SMs (144 per GPU × 72) |
| L2 Cache Size | 24 MB | 6,912 MB (96 MB per GPU × 72) |
| Max Clock Speed | 3003 MHz | ~2800 MHz |
| Shared Memory (per SM) | 100 KB | 228 KB |
| Form Factor | Desktop/workstation | Liquid-cooled rack |
| Addressing Mode | ATS (Address Translation Services) | ATS (Address Translation Services) |
| CUDA Toolkit | 13.0 | 13.0 |

### Key Architectural Difference: Compute Capability

A critical distinction between GB10 and GB200 NVL72 is their compute capability and scale:

**GB200 NVL72 (Compute Capability 10.0, 72 GPUs):**
- Part of the Hopper/Blackwell family
- 72 Blackwell GPUs with 144 SMs each = 10,368 total SMs
- Supports FP6 and FP4 Tensor Core operations
- Maximum shared memory per SM: 228 KB
- Maximum shared memory per block: 227 KB
- L2 cache configuration: 256 MB maximum per GPU
- Maximum resident threads per SM: 2048
- Maximum resident blocks per SM: 32
- 36 Grace CPUs (72-core Arm per CPU = 2,592 total cores)

**GB10 (Compute Capability 12.1, 1 GPU):**
- Part of the newer Blackwell family (12.x)
- Single GPU with ~40 SMs
- Also supports FP6 and FP4 Tensor Core operations
- Maximum shared memory per SM: 100 KB (more constrained)
- Maximum shared memory per block: 99 KB
- L2 cache configuration: 128 MB maximum
- Maximum resident threads per SM: 1536
- Maximum resident blocks per SM: 24
- 1 Grace CPU (20-core Arm)

The GB200 NVL72's massive scale provides:
- **259x more SMs** (10,368 vs 40)
- **288x more L2 cache** (6,912 MB vs 24 MB)
- **2.28x more shared memory per SM** (228 KB vs 100 KB)
- **36x more Grace CPU cores** (2,592 vs 20)

This represents the full spectrum from desktop AI development to datacenter-scale AI training and inference.

## Memory Specifications

| Feature | NVIDIA GB10 | NVIDIA GB200 NVL72 |
|---------|-------------|---------------------|
| Memory Capacity | 128 GB LPDDR5x | 13.5 TB HBM3e + 17 TB LPDDR5x = 30.5 TB total |
| GPU Memory | 128 GB (shared with CPU) | 13.5 TB HBM3e (188 GB per GPU) |
| CPU Memory | 128 GB (shared with GPU) | 17 TB LPDDR5x (472 GB per CPU) |
| Memory Bandwidth (GPU) | ~273 GB/s | ~576 TB/s (8 TB/s per GPU × 72) |
| Memory Bandwidth (CPU) | Shared with GPU | ~4.7 TB/s (130 GB/s per CPU × 36) |
| Memory Bus Width (GPU) | 256-bit | 6144-bit per GPU |
| ECC Support | Yes (system-wide) | Yes (system-wide) |
| Memory Architecture | Unified with Grace CPU | Unified within superchips + NVLink domain |

**Key Difference:** The GB200 NVL72 uses high-bandwidth HBM3e memory delivering 576 TB/s total GPU bandwidth compared to the GB10's LPDDR5x at ~273 GB/s. This massive 2,110x bandwidth advantage enables the NVL72 to train trillion-parameter models with massive batch sizes where memory access is the primary bottleneck.

## Compute Performance

| Feature | NVIDIA GB10 | NVIDIA GB200 NVL72 |
|---------|-------------|---------------------|
| FP32 Performance | ~40 TFLOPS | ~10,800 TFLOPS (150 TFLOPS × 72) |
| FP16 Performance | ~320 TFLOPS (Tensor Cores) | ~86,400 TFLOPS (1,200 TFLOPS × 72) |
| FP8 Performance | ~640 TFLOPS | ~172,800 TFLOPS (2,400 TFLOPS × 72) |
| FP4 Performance | ~1,280 TFLOPS | ~345,600 TFLOPS (4,800 TFLOPS × 72) |
| INT8 Performance | ~640 TOPS | ~172,800 TOPS (2,400 TOPS × 72) |
| Tensor Core Generation | 5th Gen (Blackwell) | 5th Gen (Blackwell) |
| CUDA Cores | ~10,240 | ~2,654,208 (36,864 × 72) |

The GB200 NVL72 delivers approximately 270x the raw compute throughput of the GB10, making it the ultimate platform for training frontier AI models at scale. The system achieves 10.8 petaFLOPS in FP32 and up to 345.6 petaFLOPS in FP4 precision.

## Power & Thermal Characteristics

| Feature | NVIDIA GB10 | NVIDIA GB200 NVL72 |
|---------|-------------|---------------------|
| GPU TDP | Part of 140W total | ~50,400W (700W × 72 GPUs) |
| CPU TDP | Part of 140W total | ~18,000W (500W × 36 CPUs) |
| Total System TDP | 140W (CPU+GPU) | ~120 kW (rack-level with cooling) |
| Idle Power | ~4.6W | ~2,000W |
| Form Factor | Dual-slot desktop | Liquid-cooled rack (9 chassis) |
| Cooling | Active air cooling | Direct liquid cooling required |
| Power Efficiency | ~229 GFLOPS/W (FP16) | ~720 GFLOPS/W (FP16) |
| Power Distribution | Single power supply | Three-phase datacenter power |

Despite the GB200 NVL72's 857x higher TDP (120 kW vs 140W), it achieves 3.1x better power efficiency (~720 vs ~229 GFLOPS/W) due to optimized HBM3e memory subsystem and NVLink interconnect reducing data movement overhead. The NVL72 requires dedicated liquid cooling infrastructure and typically consumes 120-130 kW at full load.

## Advanced Features

| Feature | NVIDIA GB10 | NVIDIA GB200 NVL72 |
|---------|-------------|---------------------|
| Multi-Instance GPU (MIG) | Yes (up to 7 instances) | Yes (up to 7 instances per GPU) |
| NVLink | NVLink 5.0 (optional) | NVLink 5.0 (130 TB/s domain bandwidth) |
| Dynamic Programming | Full support | Full support |
| Thread Block Clusters | Yes (SM 12.1 feature) | Yes (SM 10.0 feature) |
| Tensor Memory Accelerator | Yes (TMA) | Yes (TMA) |
| Hardware Coherency | Yes (ATS/PASID support) | Yes (ATS/PASID support) |
| Transformer Engine | Yes (FP8 optimized) | Yes (FP8 + FP4 optimized) |
| Distributed Shared Memory | Yes | Yes |
| Confidential Computing | Limited | Full support with TEE |
| Scale | Single desktop system | 72-GPU unified fabric |

**GB200 NVL72 Exclusive Advantages:**
- **130 TB/s NVLink Domain**: Unified fabric connecting all 72 GPUs with ultra-low latency
- **Massive Scale**: 72 GPUs operate as a single massive accelerator
- **Confidential Computing**: Trusted Execution Environment (TEE) for secure AI workloads
- **NVSwitch Integration**: 18 NVSwitches create fully-connected GPU topology
- **Liquid Cooling**: Direct liquid cooling for sustained peak performance

## Networking & Multi-Node Connectivity

| Feature | NVIDIA GB10 | NVIDIA GB200 NVL72 |
|---------|-------------|---------------------|
| High-Speed Interconnect | ConnectX-7 (200 Gb/s IB/Ethernet) | ConnectX-8 (400 Gb/s IB / 800 Gb/s Ethernet) |
| Multi-Node Clustering | Native support via ConnectX-7 | Native support via ConnectX-8 |
| NVLink Fabric | Optional | 130 TB/s internal NVLink domain |
| NVSwitch | Not included | 18 NVSwitches (full GPU interconnect) |
| RDMA Support | Integrated with ConnectX-7 | Integrated with ConnectX-8 |
| GPU-to-GPU Direct | Yes (GPUDirect RDMA) | Yes (GPUDirect RDMA + NVLink) |
| SHARP Support | Yes | Yes (enhanced) |
| Scale | Single node | 72 GPUs in unified fabric |

**GB200 NVL72 Multi-Rack Scaling:**
- **130 TB/s NVLink Domain**: All 72 GPUs connected via 18 NVSwitches
- **ConnectX-8 (400 Gb/s IB / 800 Gb/s Ethernet)**: 36 NICs for inter-rack communication
- **2-tier Network**: NVLink within rack, ConnectX-8 between racks
- **NCCL Optimizations**: Topology-aware collectives for massive-scale training
- **Rack Scale**: Single NVL72 rack can be further scaled with additional racks

**GB10 Multi-Node Scaling:**
- **ConnectX-7 Only**: 200 Gb/s per node
- **Smaller Clusters**: Typically 8-32 nodes
- **Flat Network**: All communication via ConnectX-7

## Memory Subsystem Enhancements

### NVIDIA GB10
- Hardware-coherent unified memory via ATS
- LPDDR5x shared with Grace CPU (128 GB)
- PCIe Gen 5 (64 GB/s per direction)
- ~273 GB/s memory bandwidth (256-bit bus @ 8533 MT/s)
- Optimized for moderate batch sizes

### NVIDIA GB200 NVL72
- Hardware-coherent unified memory via ATS within each superchip
- HBM3e on GPUs (13.5 TB total, 188 GB per GPU)
- LPDDR5x on CPUs (17 TB total, 472 GB per CPU)
- NVLink 5.0 domain (130 TB/s all-to-all GPU communication)
- ~576 TB/s aggregate GPU memory bandwidth
- Optimized for massive batch sizes and trillion-parameter models
- Ultra-low latency GPU-to-GPU communication

The GB200 NVL72's HBM3e provides the bandwidth necessary to keep 10,368 SMs fully utilized across 72 GPUs, while the GB10's LPDDR5x at ~273 GB/s is matched to its 40 SMs. The 2,110x bandwidth difference (576 TB/s vs 273 GB/s) reflects the extreme scaling from desktop to datacenter. Additionally, the 130 TB/s NVLink domain enables near-instantaneous model parallelism across all GPUs.

## Software & Ecosystem

| Feature | NVIDIA GB10 | NVIDIA GB200 NVL72 |
|---------|-------------|---------------------|
| CUDA Toolkit | 13.0+ | 13.0+ |
| Minimum Driver | 570.00+ | 570.00+ |
| cuBLAS | cuBLAS 13.x with FP8 | cuBLAS 13.x with FP8 |
| cuDNN | 9.x | 9.x (optimized for large scale) |
| TensorRT | 11.x+ (FP8 support) | 11.x+ (FP8 support) |
| NCCL | 2.28+ | 2.28+ (NVLink topology-aware) |
| NeMo Framework | Supported | Optimized for NVL72 |
| Megatron-LM | Supported | Primary target platform |

**Software Optimizations:**
- Both platforms benefit from CUDA 13.0 features equally
- GB200 NVL72 has additional optimizations for:
  - Extreme-scale model parallelism (tensor, pipeline, data parallel)
  - NVLink-aware collective operations across 72 GPUs
  - Multi-rack coordination for trillion-parameter models
  - Topology-aware communication patterns

## Practical Use Cases & Positioning

### NVIDIA GB10 - Desktop AI Development
**Ideal For:**
- AI model prototyping and experimentation
- Fine-tuning models up to 200B parameters (with quantization)
- Inference for models up to 70B parameters
- Individual researcher and developer workloads
- Educational institutions
- Proof-of-concept development

**Advantages:**
- Extremely low power consumption (140W total)
- Affordable entry to Blackwell architecture
- Desktop form factor
- No special infrastructure required
- Good for iterative development

**Limitations:**
- Single GPU limits model size
- Memory bandwidth ~273 GB/s
- Not suitable for large-scale training

### NVIDIA GB200 NVL72 - Hyperscale AI Training
**Ideal For:**
- Trillion-parameter frontier model training
- Real-time inference at massive scale (1.8T parameter models)
- Hyperscale cloud providers
- National AI infrastructure
- Large-scale AI research institutions
- Production AI services at extreme scale

**Advantages:**
- 30x faster real-time LLM inference vs H100
- 576 TB/s memory bandwidth
- 130 TB/s NVLink domain for model parallelism
- 13.5 TB HBM3e + 17 TB LPDDR5x = 30.5 TB total memory
- 345.6 petaFLOPS peak performance (FP4)
- Unified 72-GPU fabric

**Best Use Cases:**
- Training GPT-5 scale models (>10T parameters)
- Real-time inference for 1.8T parameter MoE models
- Multi-modal foundation model training
- Scientific computing + AI convergence at extreme scale

### When to Choose GB10 vs GB200 NVL72

**Choose GB10 when:**
- Individual developer or small team
- Budget < $30,000
- Power budget < 200W
- Desktop/office environment
- Prototyping and experimentation
- Models < 200B parameters

**Choose GB200 NVL72 when:**
- Enterprise/hyperscale deployment
- Budget > $3-5M per rack
- Datacenter infrastructure available
- Liquid cooling infrastructure
- Training models > 1T parameters
- Need for 72-GPU unified fabric
- Real-time inference at extreme scale

## Performance Impact Examples

### LLM Training (1.8T parameter MoE model, FP8)

**GB10 Cluster (256 nodes, 256 GPUs):**
- Training throughput: ~8,000 tokens/second
- Memory per GPU: 128 GB (requires extensive model sharding)
- Time to 1T tokens: ~1.4 days
- Power consumption: ~36 kW
- Complex multi-node coordination required

**GB200 NVL72 (Single rack, 72 GPUs):**
- Training throughput: ~116,000 tokens/second (30x H100 baseline)
- Memory: 13.5 TB HBM3e + 17 TB LPDDR5x (188 GB HBM3e per GPU)
- Time to 1T tokens: ~2.4 hours (14.5x faster than GB10 cluster)
- Power consumption: ~120 kW
- Unified NVLink fabric eliminates multi-node overhead

### LLM Inference (1.8T parameter MoE model, FP4 real-time)

**GB10:**
- Not feasible for this model size
- Would require 256+ GB10s with severe latency penalties

**GB200 NVL72:**
- Throughput: 116 tokens/second/GPU average
- Total: ~8,352 tokens/second across 72 GPUs
- Latency: 50ms token-to-token (real-time)
- Batch size: 32,768 input / 1,024 output tokens
- Enables real-time trillion-parameter inference

### Mixed Precision Training (GPT-4 scale, 1.76T)

**GB10 (512 GPUs across 512 nodes):**
- ~12,000 tokens/second
- FP8 Tensor Cores + BF16 accumulation
- Massive gradient accumulation required
- Network becomes primary bottleneck

**GB200 NVL72 (Single rack):**
- ~150,000 tokens/second
- FP4/FP8 Tensor Cores + BF16 accumulation
- 130 TB/s NVLink enables efficient pipeline parallelism
- 12.5x faster than distributed GB10 cluster
- Single-rack simplicity

### Memory-Bound Attention Operations

The GB200 NVL72's 576 TB/s aggregate memory bandwidth provides 2,110x advantage over GB10's 273 GB/s for memory-bound operations like attention mechanisms in transformers. The 130 TB/s NVLink domain enables efficient attention across model-parallel splits, making trillion-parameter models practical.

## Cost & Efficiency Analysis

### Total Cost of Ownership (TCO) Comparison

**GB10 Deployment (256 GPUs for comparison):**
- Hardware cost: ~$1.6-2.0M (256 units × ~$7,000)
- Power consumption: ~36 kW (256 × 140W)
- Cooling: Standard datacenter air cooling
- Rack space: 32-64U (8-16 racks)
- Network infrastructure: ConnectX-7, standard switching
- Annual power cost (at $0.10/kWh): ~$31,500

**GB200 NVL72 Deployment (Single rack, 72 GPUs):**
- Hardware cost: ~$3-5M per rack
- Power consumption: ~120 kW
- Cooling: Direct liquid cooling infrastructure required
- Rack space: Single rack (9 chassis)
- Network infrastructure: Included (130 TB/s NVLink + ConnectX-8)
- Annual power cost (at $0.10/kWh): ~$105,000

**Key Insights:**
- **Performance/Dollar**: GB200 NVL72 delivers significantly better performance per dollar despite higher upfront cost
- **Performance/Watt**: NVL72 achieves ~720 GFLOPS/W vs GB10's ~229 GFLOPS/W (3.1x better)
- **Space Efficiency**: NVL72 delivers 72 GPUs in a single rack vs GB10 requiring 8-16 racks for comparable GPU count
- **Operational Complexity**: Single NVL72 rack much simpler to manage than 256-node GB10 cluster
- **Time-to-Solution**: For frontier models, NVL72 completes training 10-15x faster, dramatically reducing opportunity cost

## Architecture Philosophy

### GB10: Democratizing AI Computing
The GB10 represents NVIDIA's strategy to bring Blackwell capabilities to individual developers and small teams:
- **Accessibility**: Entry-level pricing for Blackwell architecture
- **Simplicity**: Desktop form factor, standard power/cooling
- **Development**: Ideal for prototyping and experimentation
- **Education**: Perfect for learning AI development

### GB200 NVL72: Redefining AI Scale
The GB200 NVL72 is designed to push the absolute boundaries of AI capability:
- **Maximum Scale**: 72 GPUs as a single unified accelerator
- **Integration**: 130 TB/s NVLink domain + 18 NVSwitches
- **Capability**: Train and infer trillion-parameter models
- **Infrastructure**: Purpose-built for hyperscale datacenters
- **Performance**: 30x faster real-time inference vs previous generation

## Summary

The GB10 and GB200 NVL72 represent the complete spectrum of NVIDIA's Blackwell architecture—from desktop AI development to hyperscale datacenter deployment. The comparison illustrates the extraordinary range of AI computing from a single 140W desktop system to a 120kW rack with 72 GPUs.

**GB10 Sweet Spot:**
- Form factor: Desktop workstation
- Power: 140W total system
- Use case: Prototyping, fine-tuning, inference
- Models: Up to 200B parameters
- Budget: $7,000-10,000

**GB200 NVL72 Sweet Spot:**
- Form factor: Liquid-cooled rack (9 chassis)
- Power: ~120 kW
- Use case: Frontier model training, real-time trillion-parameter inference
- Models: 1T-10T+ parameters
- Budget: $3-5M per rack

**Shared Blackwell DNA:**
- 5th generation Tensor Cores with FP8/FP6/FP4
- Hardware-coherent unified memory (ATS)
- Thread block clusters and distributed shared memory
- Tensor Memory Accelerator (TMA)
- CUDA 13.0 feature set
- Grace CPU integration

**Key Scale Differences:**
- **GPUs**: 1 vs 72 (72x)
- **SMs**: 40 vs 10,368 (259x)
- **Memory**: 128 GB vs 30.5 TB (238x)
- **Memory BW**: 273 GB/s vs 576 TB/s (2,110x)
- **Compute**: 320 TFLOPS vs 86,400 TFLOPS (270x FP16)
- **Power**: 140W vs 120 kW (857x)
- **NVLink**: Optional vs 130 TB/s domain

**Architectural Distinction:**
- GB200 NVL72 uses compute capability 10.0 (Hopper/Blackwell family) with 228 KB shared memory per SM
- GB10 uses compute capability 12.1 (newer Blackwell family) with 100 KB shared memory per SM
- NVL72's 130 TB/s NVLink domain creates a single unified 72-GPU fabric
- GB10 represents efficient desktop AI, NVL72 represents datacenter-scale AI supercomputing

The GB10 democratizes Blackwell for developers worldwide, while the GB200 NVL72 enables frontier AI research and deployment at unprecedented scale. Together, they span the complete range from $7k desktop systems to $5M hyperscale racks.
