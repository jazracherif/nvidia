# NVIDIA GB10 vs GB200 NVL72: Blackwell Family Comparison

The GB10 and GB200 NVL72 represent two extremes of NVIDIA's Blackwell architecture deployment spectrum. While the GB10 brings Blackwell capabilities to the desktop in a compact 140W package, the GB200 NVL72 is NVIDIA's flagship AI supercomputer rack featuring 72 Blackwell GPUs and 36 Grace CPUs designed for the most demanding large-scale AI training and inference workloads at datacenter scale.

## Core Specification Comparison

| Feature | NVIDIA GB10 | NVIDIA GB200 NVL72 |
|---------|-------------|---------------------|
| Release Date | Q4 2025 | Q1 2025 |
| Architecture | Blackwell (SM 12.1) | Hopper/Blackwell (SM 10.0) |
| Compute Capability | 12.1 | 10.0 |
| GPU Count | 1 GPU | 72 GPUs |
| CPU Count | 1 Grace CPU (20-core) | 36 Grace CPUs |
| Streaming Multiprocessors | 48 SMs | ~10,368 SMs (144 per GPU × 72) |
| L2 Cache Size | 24 MB | 6,912 MB (96 MB per GPU × 72) |
| Max Clock Speed | 3003 MHz | ~2800 MHz |
| Shared Memory (per SM) | 100 KB | 228 KB |
| Form Factor | Desktop/workstation | Liquid-cooled rack |
| Addressing Mode | ATS (Address Translation Services) | ATS (Address Translation Services) |
| CUDA Toolkit | 13.0 | 13.0 |

### Key Architectural Difference: Compute Capability

A critical distinction between GB10 and GB200 NVL72 is their compute capability and scale:

**GB200 NVL72 (Compute Capability 10.0, 72 GPUs):**
- Part of the Hopper/Blackwell family
- 72 Blackwell GPUs with 144 SMs each = 10,368 total SMs
- Supports FP6 and FP4 Tensor Core operations
- Maximum shared memory per SM: 228 KB
- Maximum shared memory per block: 227 KB
- L2 cache configuration: 256 MB maximum per GPU
- Maximum resident threads per SM: 2048
- Maximum resident blocks per SM: 32
- 36 Grace CPUs with 72-core Arm Neoverse V2 per CPU = 2,592 total cores
- Individual GPU: 186 GB HBM3E with 8 TB/s bandwidth
- 7 NVDEC decoders and 7 nvJPEG per GPU
- Multi-Instance GPU (MIG): 7 instances per GPU

**GB10 (Compute Capability 12.1, 1 GPU):**
- Part of the newer Blackwell family (12.x)
- Single GPU with 48 SMs
- Also supports FP6 and FP4 Tensor Core operations
- Maximum shared memory per SM: 100 KB (more constrained)
- Maximum shared memory per block: 99 KB
- L2 cache configuration: 128 MB maximum
- Maximum resident threads per SM: 1536
- Maximum resident blocks per SM: 24
- 1 Grace CPU (20-core Arm)

The GB200 NVL72's massive scale provides:
- **216x more SMs** (10,368 vs 48)
- **288x more L2 cache** (6,912 MB vs 24 MB)
- **2.28x more shared memory per SM** (228 KB vs 100 KB)
- **129.6x more Grace CPU cores** (2,592 Arm Neoverse V2 vs 20)
- **242x more total memory** (31 TB vs 128 GB)
- **1,125x more FP4 compute** (1.44 exaFLOPS vs 1.28 PFLOPS)

This represents the full spectrum from desktop AI development to datacenter-scale AI training and inference.

## Memory Specifications

| Feature | NVIDIA GB10 | NVIDIA GB200 NVL72 |
|---------|-------------|---------------------|
| Total Fast Memory | 128 GB LPDDR5x | 31 TB (HBM3e + LPDDR5x) |
| GPU Memory | 128 GB (shared with CPU) | 13.4 TB HBM3e (186 GB per GPU × 72) |
| CPU Memory | 128 GB (shared with GPU) | ~17.6 TB LPDDR5x (489 GB per CPU × 36) |
| Memory Bandwidth (GPU) | ~273 GB/s | 576 TB/s (8 TB/s per GPU × 72) |
| Memory Bandwidth (CPU) | Shared with GPU | ~4.7 TB/s aggregate |
| Memory Bus Width (GPU) | 256-bit | 6144-bit per GPU |
| ECC Support | Yes (system-wide) | Yes (system-wide) |
| Memory Architecture | Unified with Grace CPU | Unified within superchips + NVLink domain |

**Key Difference:** The GB200 NVL72 uses high-bandwidth HBM3e memory delivering 576 TB/s total GPU bandwidth compared to the GB10's LPDDR5x at ~273 GB/s. This massive 2,110x bandwidth advantage enables the NVL72 to train trillion-parameter models with massive batch sizes where memory access is the primary bottleneck.

## Compute Performance

| Feature | NVIDIA GB10 | NVIDIA GB200 NVL72 |
|---------|-------------|---------------------|
| FP4 Performance | ~1.28 PFLOPS | 1,440 PFLOPS (1.44 exaFLOPS) |
| FP8/FP6 Performance | ~640 TFLOPS | 720 PFLOPS |
| FP16/BF16 Performance | ~320 TFLOPS (Tensor Cores) | 360 PFLOPS (5 PFLOPS × 72) |
| TF32 Performance | ~160 TFLOPS | 180 PFLOPS (2.5 PFLOPS × 72) |
| FP32 Performance | ~40 TFLOPS | 5.76 PFLOPS (80 TFLOPS × 72) |
| FP64 Performance | ~20 TFLOPS | 2.88 PFLOPS (40 TFLOPS × 72) |
| INT8 Performance | ~640 TOPS | 720 POPS (10 POPS × 72) |
| Tensor Core Generation | 5th Gen (Blackwell) | 5th Gen (Blackwell) |
| CUDA Cores | ~10,240 | ~2,654,208 (36,864 × 72) |

The GB200 NVL72 delivers extraordinary compute throughput: 1.44 exaFLOPS in FP4 precision (1,125x more than GB10) and 720 petaFLOPS in FP8. This makes it the ultimate platform for training frontier AI models at extreme scale.

## Power & Thermal Characteristics

| Feature | NVIDIA GB10 | NVIDIA GB200 NVL72 |
|---------|-------------|---------------------|
| Total Rack TDP | 140W (CPU+GPU) | ~132 kW (entire rack system) |
| Max Configurable TDP | 140W | Up to 1,200W (per component) |
| Idle Power | ~4.6W | ~2,000W (estimated) |
| Form Factor | Dual-slot desktop | Liquid-cooled rack (9 chassis) |
| Cooling | Active air cooling | Direct liquid cooling required |
| Power Efficiency | ~229 GFLOPS/W (FP16) | ~273 GFLOPS/W (FP16, 360 PFLOPS / 132 kW) |
| Power Distribution | Single power supply | Three-phase datacenter power |

The GB200 NVL72 rack system has a total TDP of approximately 132 kW, representing a 943x increase over GB10's 140W. Despite the massive power consumption, the system achieves excellent power efficiency at ~273 GFLOPS/W (FP16 basis), delivering 1.44 exaFLOPS of FP4 compute performance. The direct liquid cooling infrastructure is essential for sustained peak performance across all 72 GPUs and 36 Grace CPUs.

## Advanced Features

| Feature | NVIDIA GB10 | NVIDIA GB200 NVL72 |
|---------|-------------|---------------------|
| Multi-Instance GPU (MIG) | Yes (up to 7 instances) | Yes (up to 7 instances per GPU) |
| NVLink | NVLink 5.0 (optional) | NVLink 5.0 (130 TB/s domain bandwidth) |
| Dynamic Programming | Full support | Full support |
| Thread Block Clusters | Yes (SM 12.1 feature) | Yes (SM 10.0 feature) |
| Tensor Memory Accelerator | Yes (TMA) | Yes (TMA) |
| Hardware Coherency | Yes (ATS/PASID support) | Yes (ATS/PASID support) |
| Transformer Engine | Yes (FP8 optimized) | Yes (FP8 + FP4 optimized) |
| Distributed Shared Memory | Yes | Yes |
| Confidential Computing | Limited | Full support with TEE |
| Scale | Single desktop system | 72-GPU unified fabric |

**GB200 NVL72 Exclusive Advantages:**
- **130 TB/s NVLink Domain**: Unified fabric connecting all 72 GPUs with ultra-low latency
- **Massive Scale**: 72 GPUs operate as a single massive accelerator
- **Confidential Computing**: Trusted Execution Environment (TEE) for secure AI workloads
- **NVSwitch Integration**: 18 NVSwitches create fully-connected GPU topology
- **Liquid Cooling**: Direct liquid cooling for sustained peak performance

## Networking & Multi-Node Connectivity

| Feature | NVIDIA GB10 | NVIDIA GB200 NVL72 |
|---------|-------------|---------------------|
| High-Speed Interconnect | ConnectX-7 (200 Gb/s IB/Ethernet) | ConnectX-8 (400 Gb/s IB / 800 Gb/s Ethernet) |
| Multi-Node Clustering | Native support via ConnectX-7 | Native support via ConnectX-8 |
| NVLink Fabric | Optional | 130 TB/s internal NVLink domain |
| NVSwitch | Not included | 18 NVSwitches (full GPU interconnect) |
| RDMA Support | Integrated with ConnectX-7 | Integrated with ConnectX-8 |
| GPU-to-GPU Direct | Yes (GPUDirect RDMA) | Yes (GPUDirect RDMA + NVLink) |
| SHARP Support | Yes | Yes (enhanced) |
| Scale | Single node | 72 GPUs in unified fabric |

**GB200 NVL72 Multi-Rack Scaling:**
- **130 TB/s NVLink Domain**: All 72 GPUs connected via 18 NVSwitches
- **ConnectX-8 (400 Gb/s IB / 800 Gb/s Ethernet)**: 36 NICs for inter-rack communication
- **2-tier Network**: NVLink within rack, ConnectX-8 between racks
- **NCCL Optimizations**: Topology-aware collectives for massive-scale training
- **Rack Scale**: Single NVL72 rack can be further scaled with additional racks

**GB10 Multi-Node Scaling:**
- **ConnectX-7 Only**: 200 Gb/s per node
- **Smaller Clusters**: Typically 8-32 nodes
- **Flat Network**: All communication via ConnectX-7

## Memory Subsystem Enhancements

### NVIDIA GB10
- Hardware-coherent unified memory via ATS
- LPDDR5x shared with Grace CPU (128 GB)
- PCIe Gen 5 (64 GB/s per direction)
- ~273 GB/s memory bandwidth (256-bit bus @ 8533 MT/s)
- Optimized for moderate batch sizes

### NVIDIA GB200 NVL72
- Hardware-coherent unified memory via ATS within each superchip
- HBM3e on GPUs (13.4 TB total, 186 GB per GPU)
- LPDDR5x on CPUs (~17.6 TB total, 489 GB per CPU)
- NVLink 5.0 domain (130 TB/s all-to-all GPU communication)
- 576 TB/s aggregate GPU memory bandwidth (8 TB/s per GPU)
- Optimized for massive batch sizes and trillion-parameter models
- Ultra-low latency GPU-to-GPU communication via NVLink
- Total fast memory: 31 TB

The GB200 NVL72's HBM3e provides the bandwidth necessary to keep 10,368 SMs fully utilized across 72 GPUs, while the GB10's LPDDR5x at ~273 GB/s is matched to its 48 SMs. The 2,110x bandwidth difference (576 TB/s vs 273 GB/s) reflects the extreme scaling from desktop to datacenter. The 130 TB/s NVLink domain enables near-instantaneous model parallelism across all GPUs.

## Software & Ecosystem

| Feature | NVIDIA GB10 | NVIDIA GB200 NVL72 |
|---------|-------------|---------------------|
| CUDA Toolkit | 13.0+ | 13.0+ |
| Minimum Driver | 570.00+ | 570.00+ |
| cuBLAS | cuBLAS 13.x with FP8 | cuBLAS 13.x with FP8 |
| cuDNN | 9.x | 9.x (optimized for large scale) |
| TensorRT | 11.x+ (FP8 support) | 11.x+ (FP8 support) |
| NCCL | 2.28+ | 2.28+ (NVLink topology-aware) |
| NeMo Framework | Supported | Optimized for NVL72 |
| Megatron-LM | Supported | Primary target platform |

**Software Optimizations:**
- Both platforms benefit from CUDA 13.0 features equally
- GB200 NVL72 has additional optimizations for:
  - Extreme-scale model parallelism (tensor, pipeline, data parallel)
  - NVLink-aware collective operations across 72 GPUs
  - Multi-rack coordination for trillion-parameter models
  - Topology-aware communication patterns

## Practical Use Cases & Positioning

### NVIDIA GB10 - Desktop AI Development
**Ideal For:**
- AI model prototyping and experimentation
- Fine-tuning models up to 200B parameters (with quantization)
- Inference for models up to 70B parameters
- Individual researcher and developer workloads
- Educational institutions
- Proof-of-concept development

**Advantages:**
- Extremely low power consumption (140W total)
- Affordable entry to Blackwell architecture
- Desktop form factor
- No special infrastructure required
- Good for iterative development

**Limitations:**
- Single GPU limits model size
- Memory bandwidth ~273 GB/s
- Not suitable for large-scale training

### NVIDIA GB200 NVL72 - Hyperscale AI Training
**Ideal For:**
- Trillion-parameter frontier model training
- Real-time inference at massive scale (1.8T parameter models)
- Hyperscale cloud providers
- National AI infrastructure
- Large-scale AI research institutions
- Production AI services at extreme scale

**Advantages:**
- 30x faster real-time LLM inference vs H100
- 576 TB/s memory bandwidth
- 130 TB/s NVLink domain for model parallelism
- 31 TB total fast memory (13.4 TB HBM3e + 17.6 TB LPDDR5x)
- 1.44 exaFLOPS peak performance (FP4)
- Unified 72-GPU fabric

**Best Use Cases:**
- Training GPT-5 scale models (>10T parameters)
- Real-time inference for 1.8T parameter MoE models
- Multi-modal foundation model training
- Scientific computing + AI convergence at extreme scale

### When to Choose GB10 vs GB200 NVL72

**Choose GB10 when:**
- Individual developer or small team
- Budget < $10,000
- Power budget < 200W
- Desktop/office environment
- Prototyping and experimentation
- Models < 200B parameters

**Choose GB200 NVL72 when:**
- Enterprise/hyperscale deployment
- Budget > $3-5M per rack
- Datacenter infrastructure available
- Liquid cooling infrastructure
- Training models > 1T parameters
- Need for 72-GPU unified fabric
- Real-time inference at extreme scale

## Performance Impact Examples

### LLM Training (1.8T parameter MoE model, FP8)

**GB10 Cluster (1,125 nodes, 1,125 GPUs - compute-matched):**
- Training throughput: ~35,000 tokens/second
- Memory per GPU: 128 GB (requires extensive model sharding)
- Time to 1T tokens: ~8 hours
- Power consumption: ~157.5 kW
- Complex multi-node coordination required

**GB200 NVL72 (Single rack, 72 GPUs):**
- Training throughput: ~116,000 tokens/second (30x H100 baseline)
- Memory: 31 TB total (13.4 TB HBM3e + 17.6 TB LPDDR5x)
- 186 GB HBM3e per GPU with 8 TB/s bandwidth
- Time to 1T tokens: ~2.4 hours (3.3x faster than compute-matched GB10 cluster)
- Power consumption: ~132 kW (total rack TDP, 16% less than GB10 cluster)
- Unified NVLink fabric eliminates multi-node overhead

### LLM Inference (1.8T parameter MoE model, FP4 real-time)

**GB10:**
- Not feasible for this model size
- Would require 1,125+ GB10s with severe latency penalties

**GB200 NVL72:**
- Throughput: 116 tokens/second/GPU average
- Total: ~8,352 tokens/second across 72 GPUs
- Latency: 50ms token-to-token (real-time)
- Batch size: 32,768 input / 1,024 output tokens
- Enables real-time trillion-parameter inference

### Mixed Precision Training (GPT-4 scale, 1.76T)

**GB10 (1,125 GPUs across 1,125 nodes - compute-matched):**
- ~26,000 tokens/second
- FP8 Tensor Cores + BF16 accumulation
- Massive gradient accumulation required
- Network becomes primary bottleneck

**GB200 NVL72 (Single rack):**
- ~150,000 tokens/second
- FP4/FP8 Tensor Cores + BF16 accumulation
- 130 TB/s NVLink enables efficient pipeline parallelism
- 5.8x faster than compute-matched GB10 cluster
- Single-rack simplicity

### Memory-Bound Attention Operations

The GB200 NVL72's 576 TB/s aggregate memory bandwidth provides 2,110x advantage over GB10's 273 GB/s for memory-bound operations like attention mechanisms in transformers. The 130 TB/s NVLink domain enables efficient attention across model-parallel splits, making trillion-parameter models practical.

## Cost & Efficiency Analysis

This analysis compares the total cost of ownership between a compute-matched GB10 cluster and a single GB200 NVL72 rack. To achieve FP4 performance parity with the GB200 NVL72's 1,440 PFLOPS (1.44 exaFLOPS), we need 1,125 GB10 systems (1,440 PFLOPS ÷ 1.28 PFLOPS per GB10 = 1,125). This comparison reveals the economic, operational, and performance trade-offs between distributed desktop systems and integrated rack-scale infrastructure.

### Total Cost of Ownership (TCO) Comparison

| Metric | GB10 Deployment (1,125 GPUs) | GB200 NVL72 Deployment (72 GPUs) |
|--------|------------------------------|----------------------------------|
| **GPU Count** | 1,125 GPUs (compute-matched for FP4) | 72 GPUs (single rack) |
| **Hardware Cost** | ~$4.5M (1,125 × $4,000) | ~$3-5M per rack |
| **Power Consumption** | ~157.5 kW (1,125 × 140W) | ~132 kW (total rack TDP) |
| **Cooling** | Standard datacenter air cooling | Direct liquid cooling required |
| **Rack Space** | 112-140 racks | Single rack (9 chassis) |
| **Network Infrastructure** | ConnectX-7, extensive switching required | Included (130 TB/s NVLink + ConnectX-8) |
| **Annual Power Cost** | ~$138,000 (at $0.10/kWh) | ~$116,000 (at $0.10/kWh) |

**Key Insights:**
- **Performance/Dollar**: GB200 NVL72 ($3-5M) delivers competitive value compared to compute-matched GB10 cluster ($4.5M)
- **Performance/Watt**: NVL72 achieves ~273 GFLOPS/W vs GB10's ~229 GFLOPS/W, with 16% lower power (132 kW vs 157.5 kW)
- **Space Efficiency**: NVL72 delivers equivalent FP4 compute in a single rack vs 112-140 racks for GB10s
- **Operational Complexity**: Single NVL72 rack dramatically simpler to manage than 1,125-node GB10 cluster
- **Time-to-Solution**: For frontier models, NVL72 completes training 3-4x faster despite compute parity due to superior memory bandwidth and interconnect
- **Total Fast Memory**: 31 TB (242x more than single GB10) enables unprecedented model sizes in a single rack
- **Memory Bandwidth Advantage**: 576 TB/s vs 307 GB/s aggregate (1,875x) - critical bottleneck for large models
- **Network Simplicity**: 130 TB/s unified NVLink domain vs complex multi-rack ConnectX-7 fabric
- **Space Efficiency**: NVL72 delivers equivalent FP4 compute in a single rack vs 112-140 racks for GB10s
- **Operational Complexity**: Single NVL72 rack dramatically simpler to manage than 1,125-node GB10 cluster
- **Time-to-Solution**: For frontier models, NVL72 completes training 3-4x faster despite compute parity due to superior memory bandwidth and interconnect
- **Total Fast Memory**: 31 TB (242x more than single GB10) enables unprecedented model sizes in a single rack
- **Memory Bandwidth Advantage**: 576 TB/s vs 307 GB/s aggregate (1,875x) - critical bottleneck for large models
- **Network Simplicity**: 130 TB/s unified NVLink domain vs complex multi-rack ConnectX-7 fabric

## Architecture Philosophy

### GB10: Democratizing AI Computing
The GB10 represents NVIDIA's strategy to bring Blackwell capabilities to individual developers and small teams:
- **Accessibility**: Entry-level pricing for Blackwell architecture
- **Simplicity**: Desktop form factor, standard power/cooling
- **Development**: Ideal for prototyping and experimentation
- **Education**: Perfect for learning AI development

### GB200 NVL72: Redefining AI Scale
The GB200 NVL72 is designed to push the absolute boundaries of AI capability:
- **Maximum Scale**: 72 GPUs as a single unified accelerator
- **Integration**: 130 TB/s NVLink domain + 18 NVSwitches
- **Capability**: Train and infer trillion-parameter models
- **Infrastructure**: Purpose-built for hyperscale datacenters
- **Performance**: 30x faster real-time inference vs previous generation

## Summary

The GB10 and GB200 NVL72 represent the complete spectrum of NVIDIA's Blackwell architecture—from desktop AI development to hyperscale datacenter deployment. The comparison illustrates the extraordinary range of AI computing from a single 140W desktop system to a 120kW rack with 72 GPUs.

**GB10 Sweet Spot:**
- Form factor: Desktop workstation
- Power: 140W total system
- Use case: Prototyping, fine-tuning, inference
- Models: Up to 200B parameters
- Budget: $4,000

**GB200 NVL72 Sweet Spot:**
- Form factor: Liquid-cooled rack (9 chassis)
- Power: ~132 kW (total rack TDP)
- Use case: Frontier model training, real-time trillion-parameter inference
- Models: 1T-10T+ parameters
- Budget: $3-5M per rack
- Performance: 1.44 exaFLOPS (FP4), 720 PFLOPS (FP8)

**Shared Blackwell DNA:**
- 5th generation Tensor Cores with FP8/FP6/FP4
- Hardware-coherent unified memory (ATS)
- Thread block clusters and distributed shared memory
- Tensor Memory Accelerator (TMA)
- CUDA 13.0 feature set
- Grace CPU integration

**Key Scale Differences:**
- **GPUs**: 1 vs 72 (72x)
- **SMs**: 48 vs 10,368 (216x)
- **Memory**: 128 GB vs 31 TB (242x)
- **Memory BW**: 273 GB/s vs 576 TB/s (2,110x)
- **FP4 Compute**: 1.28 PFLOPS vs 1.44 exaFLOPS (1,125x)
- **FP16 Compute**: 320 TFLOPS vs 360 PFLOPS (1,125x)
- **Power**: 140W vs ~132 kW (943x)
- **NVLink**: Optional vs 130 TB/s domain

**Architectural Distinction:**
- GB200 NVL72 uses compute capability 10.0 (Hopper/Blackwell family) with 228 KB shared memory per SM
- GB10 uses compute capability 12.1 (newer Blackwell family) with 100 KB shared memory per SM
- NVL72's 130 TB/s NVLink domain creates a single unified 72-GPU fabric
- GB10 represents efficient desktop AI, NVL72 represents datacenter-scale AI supercomputing

The GB10 democratizes Blackwell for developers worldwide, while the GB200 NVL72 enables frontier AI research and deployment at unprecedented scale. Together, they span the complete range from $7k desktop systems to $5M hyperscale racks.
